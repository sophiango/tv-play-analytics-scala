
# ğŸ“º TV Play Analytics with Apache Flink, Kafka, and Iceberg

This project demonstrates a real-time streaming data pipeline using:
- Apache Kafka for message ingestion
- Apache Flink (Scala) for processing and transformation
- Apache Iceberg as the data lake storage format
- Flink Table API and SQL for stream-table abstraction

The end goal is to read TV play events from Kafka, parse and transform them with Flink, and write them into partitioned Iceberg tables.

---

## ğŸ§± Architecture

```
TV Events â†’ Kafka â†’ Flink (Scala + Table API) â†’ Iceberg (via Hadoop or REST catalog)
```

---

## ğŸ“‚ Project Structure

```
tv_play_analytics/
â”œâ”€â”€ docker-compose.yml         # Services: Kafka, Flink, Iceberg REST catalog
â”œâ”€â”€ flink-job/
â”‚   â”œâ”€â”€ build.sbt              # Dependencies (Flink, Iceberg)
â”‚   â””â”€â”€ src/main/scala/
â”‚       â”œâ”€â”€ Main.scala         # Job logic: Kafka â†’ Parse â†’ TableEnv SQL â†’ Iceberg sink
â”‚       â””â”€â”€ TVPlayEvent.scala  # Case class and deserializer
|		â””â”€â”€ TVPlayEventDeserializer.scala
â”œâ”€â”€ warehouse/         # Iceberg table warehouse (volume)
â””â”€â”€ README.md
```

---

## ğŸ” What Works

- âœ… Kafka consumer working inside Flink
- âœ… Flink TableEnvironment creation and SQL execution
- âœ… Kafka source table defined via SQL
- âœ… Parsing Kafka JSON events to case classes (`TVPlayEvent`)
- âœ… Dockerized Flink/Kafka integration with volume mounting

---

## âš ï¸ What Didn't Work

### âŒ Iceberg Integration with Flink (Scala)
Despite multiple attempts, Flink failed to find the Iceberg catalog factory.

#### Problems Encountered:
- `"Could not find factory for identifier 'iceberg'"` error
- ClassNotFound exceptions for Hadoop classes like `org.apache.hadoop.conf.Configuration`
- Missing shaded JARs (`iceberg-flink-runtime`, `flink-table-planner-blink`, etc.)
- Dependency hell trying to align Flink, Iceberg, and Hadoop versions

### ğŸ§¨ Attempts Made
- Built custom Dockerfile with JARs copied into `/opt/flink/lib`
- Used both `catalog-type=hadoop` and `catalog-type=rest`
- Tried both Maven Central and Icebergâ€™s own repo
- Mounted JARs into Flink JobManager container manually
- Switched between Scala and PyFlink â€” same missing JAR/class issues

---

## ğŸ§ª Next Steps (Recommended)

ğŸ’¡ Instead of continuing with Scala + fat JAR:

### âœ… Use Flink SQL Client + Docker
- Write all logic using Flink SQL DDL + INSERT INTO statements
- Mount `.sql` files and run from CLI using `flink-sql-client`
- Avoid classpath and build issues entirely

---

## ğŸ™ Lessons Learned

- Apache Flink + Iceberg integration requires **perfect dependency alignment** and shaded JARs
- Even for `catalog-type=rest`, Flink requires Iceberg JARs with Hadoop transitive deps
- PyFlink and Scala both suffer from JAR management friction
- Flink SQL Client + Docker is a great low-friction alternative

---

## ğŸ‘©â€ğŸ’» Author

**Sophia Ngo** â€” *Data Engineering Enthusiast, Full-Stack Explorer*
[ğŸŒ sophiango.site](https://sophiango.site) | ğŸ“§ ngo_sophia@icloud.com

---

## ğŸ“Œ Project Goal

Build an end-to-end portfolio piece demonstrating:
- Kafka stream processing
- Flink Table API / SQL pipeline
- Modern data lake writing using Apache Iceberg

Despite technical blockers, the core structure and learnings are still valuable.

---

## ğŸ³ How to Run (Partial Working)

```bash
# Start Kafka, Flink, etc.
docker compose up -d

# Submit fat JAR (if working)
http://localhost:8081/#/submit
```

---

## ğŸ—‚ï¸ TODO (If Revisited)

- [ ] Switch to Flink SQL client and `.sql` file orchestration
- [ ] Use MinIO + Iceberg REST catalog setup
- [ ] Add Grafana + Prometheus monitoring
- [ ] Produce mock TV events continuously to Kafka
